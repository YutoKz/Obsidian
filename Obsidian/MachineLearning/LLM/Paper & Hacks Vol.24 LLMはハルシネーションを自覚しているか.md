
LLMのハルシネーションは2種類
- 事実性のハルシネーション
	- LLMが生成した文章が事実と異なる
- 忠実性のハルシネーション
	- ユーザーの指示に従わない
	- 論理的に間違っている

LLMの内部状態とハルシネーションの関係
- 9.11 を 9.2より大きいと出力してしまう原因は、日付だと理解していたからだと判明
	- 日付に関係するニューロンを抑制することで対応可能
- 内部状態の解析方法
	- Attentionのかかり方
	- Probeを使う
	- Sparse Auto Encoder

[今日の論文：Physics of Language Models: Part2.2 How to Learn From Mistakes on Grade-School Math Problems](https://arxiv.org/abs/2408.16293)
LLMの内部状態を解析することでハルシネーションを抑制しよう

概要
- 算数の問題に対するLLMの誤り訂正能力を高めるために、誤りとその修正を含むデータを事前学習に組みこんで効果を検証
	- LLMは「誤りがあったかもしれない」という"後悔"を示す内部状態を持つ
	- "後悔"を基に修正する"retry upon regret"手法を提案
	- ある程度は誤りを検知・修正する能力を持っている
	- 一方、推論の複雑さはマシ、エラー検知が非常に高精度でなければ効果は限定的