### 第1回 生成モデル概要
- 深層生成モデルの理論背景含め、基礎知識を網羅的に扱う
	- 論文を読む基本的な知識が身につく
	- 最終回では深層生成モデルの発展である世界モデルを扱う
	- 生成AIのテクニック（拡散モデルのハイパラ調整、プロンプトエンジニアリング, etc）は対象外
- 今回は"生成モデル"
##### 生成モデル
- 顔画像は、仮想的な顔画像生成器によって生成される画像　と定義できる
	- つまり、
	  顔画像生成器の性質がわかれば、顔画像に関する全ての知識が得られる
- 顔画像生成器を定式化
	- 確率分布で定式化、データ分布$p_{data}(x)$
	- 存在をあくまで仮定したもので、形状も不明
	- これを近似したい
- パラメータ付き確率分布、確率モデル$p_\theta(x)$
	- データ分布と同じ確率モデルを作るのが目標
	- 確率モデルの設計が生成モデルでは重要
- データ分布を再現した確率モデルを　$x$の生成モデル　という
- 作ることで対象を「理解」する＝**構成論的アプローチ**
- 生成モデルでできること
	- 生成
	- 密度推定
		- データを入力すると、それがどれだけ生成モデルと違うかがわかる
		- 外れ値検出, 異常検出に
	- 欠損値補完, ノイズ除去
##### 生成モデルの学習
- 目標：データ分布に近似したい
	- 設計＝確率モデルの構成
	- 学習＝パラメータの調整
- 課題
	1. 「対象」目標のデータ分布が未知
	1. 「基準」目標までの距離をどうはかるか
- 課題1
	- データ分布は実際には得られない
	- 手元にあるのは、データ分布からサンプリングされたデータ集合(訓練集合) $\{ x_i \}_{i=1}^N$のみ
	- データ分布の代わりに、データ集合で決まる経験分布を定義
		- $\hat{p}_{data}^N(x) \equiv \frac{1}{N}\sum_{i=1}^N{\delta (x-x_i)}$
		- $\delta(x)$はディラックのデルタ分布
		- データ数Nが増えると、真のデータ分布に収束する
- 課題2
	- KLダイバージェンス$$D_{KL}[p(x)||q(x)]\equiv \mathbb{E}_{p(x)}[log \frac{p(x)}{q(x)}]$$
		- 任意のp, qについて正
		- $p(x)=q(x) \Leftrightarrow D_{KL}[p(x)||q(x)]=0$
		- $D_{KL}[p(x)||q(x)] \neq D_{KL}[q(x)||p(x)]$　となり対称性を満たさない。厳密な意味での「距離」ではない
	- 他にも距離を定義する尺度は存在
		- 他のダイバージェンスについては第3章 GAN
- 生成モデルの学習
	- 経験分布と生成モデルのKLダイバージェンスを最小化$$\hat{\theta}=arg\ min_\theta D_{KL}[\hat{p}_{data}^N(x)||p_\theta(x)]=\mathbb{E}_{\hat{p}_{data}^N}[log\ \hat{p}_{data}^N(x)]-\mathbb{E}_{\hat{p}_{data}^N}[log\ p_\theta(x)]$$
	-  第2項のみが$\theta$依存で、$\frac{1}{N} \sum_{i=1}^{N}log\   p_\theta(x_i)$と表せるので、
	  生成モデル$p_\theta(x)$の学習は$$\hat{\theta}=arg\ max_\theta\ \sum_{i=1}^Nlog\ p_\theta(x_i)$$
	  となるパラメータ$\hat{\theta}$を求めること。これはどう解釈できるか。
	- 尤度関数
		- 確率分布$p_\theta(x)$をパラメータ$\theta$の関数$f_x(\theta)$と捉え直すと、この関数$f_x(\theta)$を尤度関数という
		- 特定のxのもとでのパラメータ$\theta$の尤もらしさを表す
		- 尤度が最も高いパラメータを選んだときが、最も尤もらしい生成モデルとなるはず＝最尤推定
	- KLダイバージェンスの最小化 と 最尤推定 は等価
		- 各データが独立に生成されると仮定すると、データ集合全てにおける尤度関数は、$f_{x_i}(\theta)$ (i=1,2,3, ... N) の積で表せる
		- 対数を取って和の形に
		- この**対数尤度**の総和を最大化するパラメータを決定する
	- 生成モデルの最尤推定の注意点
		- あくまで経験損失を最小化している
			- 本当に行いたいのは汎化損失の最小化
			- 経験損失最小化が、汎化損失最小化につながるかは、
			  経験分布がどれだけ真のデータ分布を近似するか、に依存
				- その精度はデータ数Nに依存
				- データ数少ないと、経験分布の近似が不十分となり、過学習に
		- 最尤推定では、KLダイバージェンスを統計的距離として扱っている
			- KLダイバージェンスの非対称性などの性質によって、学習の結果に影響が出る

あとは
- 困難な対数尤度関数をどう最大化するか
- モデルをどう設計するか

##### 潜在変数モデルと混合モデル
- 潜在変数を含めた生成モデル：潜在変数モデル
- カテゴリ分布
	- ![[深層生成モデル_カテゴリ分布.png]]
- **条件付き分布の表記**![[深層生成モデル_条件付き確率.png]]
	- $\boldsymbol{z}$はone-hotベクトル$$\sum_{k=1}^Kz_k=1, z_k \in \{ 0,1 \}$$
	- よく見る太字の確率変数は、one-hotベクトルと捉えるといい場合があるのか
- 観測データ集合の生成モデルの生成過程を仮定
	- 事前分布から潜在変数をサンプリング$$\boldsymbol{z}_i \sim p_\pi(\boldsymbol{z})=Cat(\boldsymbol{z};\boldsymbol{\pi})$$
	- 選択したzでxを観測$$x_i\sim p_\mu(x|\boldsymbol{z}=\boldsymbol{z}_i)=Bern(x;\mu_k)$$
	  なお、kは$\boldsymbol{z}_i$に対応するカテゴリ($k=arg\ max\ \boldsymbol{z}_i$)
- EMアルゴリズム
- MNIST画像の生成モデルを学習、クラスタリング
	- 観測変数：手書き数字画像
	- 潜在変数：どの数字か（カテゴリ変数）
	- 各$z_k$の事後確率 $p_{\mu, \pi} (z_k=1|\boldsymbol{x}) \equiv \gamma(z_{ik})$を負担率とよび、これを比較することで$z_k$を推定できる
- 推論
	- 観測変数から潜在変数の分布を求めること
	- 結果から原因を探る
- 識別モデル と 生成モデル
	- 識別モデル
		- $p(y|x)$を学習
		- 学習した$p(y|x)$をもちいてxからyを予測
	- 生成モデル
		- 同時分布$p(x,y)$を学習
		- $p(y|x)$を求めることもでき、xからyを予測できる
	- 違い
		- 生成モデルから識別モデルを求めることはできるが、逆は無理
		- 生成モデルは入力(x)の分布を含めてモデル化
### 
