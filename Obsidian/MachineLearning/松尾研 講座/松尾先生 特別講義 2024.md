
grokking
train lossが100に達してから遥かに学習を進めると、ある時急に汎化性能が激増する
重みの学習ではなく、構造の学習なのではないか

今のAIの限界
counting数えるのが苦手, 正しい指の本数をもつ画像

世界モデル
人間は経験からモデルを学習
自己教師あり学習が重要
xだけから便宜的にx,yを作る
画像の一部を隠して、他の部分からそれを当てる
われわれも日常で予測と答え合わせを繰り返している

GPTの限界
意味がわかっていない
苦手なタイプの問題もたくさん
学習データにないことは、人間ならできる推測ができない

ソシュール　シニフィアン　シニフィエ
llmは記号は使えてもその指す内容は扱えない

意味を理解するとは
知覚を元に世界モデルを構築している
それを「言葉」が呼び出す。言葉で世界モデルを操る
世界モデルをある種のシミュレータとして使い、言語化する
動物osという世界モデルの入出力は知覚
それを呼び出す言語アプリが言語的入出力を担当
これが二階建ての知能

できるのか？
世界モデルの入力はセンサ、出力はアクチュエータ
言語アプリはいずれも言語
まだできない理由
高性能な世界モデルがまだないことと、世界モデルなど他のモデルを参照する方法がない

現状の課題
nnのこうぞうを先に決めてること
誤差逆伝播

本来あるべき姿
アーキテクチャが少しずつできていくこと
適当に組んだネットワークで自己教師あり学習、重要なニューロンだけ残していくと、
世界モデルを作成できる　NAM と呼び、取り組んでいる

実現すると
脳の働きや意識をうまく説明できるようになる
大脳皮質は自己教師あり学習の器官

今のAIの限界はかなり根本的なところに起因
これをNAMで解決できないか


ネットワークのアーキテクチャ
そもそも人間が決めるのがナンセンス
大脳皮質の各部位のニューロンをみると、ネットのアーキテクチャは本来データの性質等に合わせアルゴリズム的に獲得されるもの

今の学習はデータをいっぺんに用意する
その原因は誤差逆伝播、およびアーキテクチャの事前決定


全体
脳科学と絡め、人工知能と言うものを人間の脳の働きを再現、あるいは理解の助けとなるものとして考えているのが伝わった
習慣の形成過程と絡めて何か面白いことできないかな、と感じた。
