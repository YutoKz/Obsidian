# 2. Prompting と RAG
- Promptingとは
	- 特定の機能の発生を促進するような言語モデルに入力するコンテキスト文
	- Zero-shot, Few-Shot, etc
	- 文脈内学習とも
- **プロンプト文を工夫**
	- Chain-of-Thought
		- 思考過程をプロンプトにいれる
		- モデルサイズが大きいほど性能向上
		- Let's think step by step いれるだけでも（zero-shot ICL）
	- Plan-and-Solve
	- 自動生成
- **Decodingを工夫**
	- Self-Consistency
		- 複数推論させ、多数決で出力を決定
	- Tree-of-Thought



# 3. Pre-training
transformerの基礎

# 4. Scaling Law
### はじめに
- 言語モデルをスケール、すなわち大規模化する意義とは
- スケール則とは
- 推論時のスケーリングとは
- （演習）スケール則の求め方

### スケール則とは
- スケール＝大規模化
- 近年の急速なパラメータ数の増加の背景にあるのがスケール則
	- 「Scaling Laws for Neural Language Models」
	- 「Training Compute-Optimal Large Language Models」
- スケール則とは　
	- 計算資源/データセットサイズ/パラメータ数 と テスト誤差の間に成立する経験則
	- 両対数グラフ上での線形関係
	- 計算資源とテスト誤差
		- 計算資源 と モデルサイズ はほぼ同義（？）
		- 目標の誤差達成に最適な計算資源や、手元の資源で達成可能な性能が予測可能
- モデル学習に必要な計算量・計算時間の概算
	- 6 x パラメータ数 x 学習データトークン数 ＝ ??? FLOPs
	- FLOPs: 浮動小数点演算の回数
	- これを計算環境の計算能力で割ればええ
		- 例）A100: E+14 FLOPS
		- FLOP"S"は単位時間あたりの計算量なので注意
- Transformer以外のモデルでも確認されている

### スケール則の使い方
- 目標よりも小さい規模のモデルの結果から、目標の規模のモデル性能を予測できる
- スケールさせたとき、どちらのモデルのほうが有望かを予測できる
- Chinchilla則
	- 計算量を固定したとき、パラメータ数/トークン数を変動させた結果
	- ![[Pasted image 20250106175206.png]]
	- Chinchilla Optimal: 最適トークン数 ＝ 20 x パラメータ数
- 活用方法の大別
	- 投資判断：より計算機に投資すべきか
	- 効率的なモデル選択：スケールしたときの予測性能を比較
	- 効率的な計算資源活用：ChinChilla則によるトークン数とパラメータ数決定

### スケール則の測り方
基本的には、小さな条件でいくつか実験→Fittingして目標の条件を予測
ここで疑問
- モデルサイズどう変化させる？
	- 層数増やす
	- 埋め込み次元増やす
	- FFNの中間層次元増やす
	- ヘッド数増やす
- モデルサイズを大きくするとき、ハイパラはどう変える？

### 推論時のスケーリング

##### モチベーション
 「バナナの色は？」vs「スケール則の問題は？」
 以上の２つの問は、必要な思考のプロセス・負荷が異なる
 → **これらの違いをLLMでどう実現するか、そしてそれは効果的か**

A. 効果的
- 推論時に計算量を増やすと性能上がる（？）

推論時にどう計算量をスケールさせるか
- Promptingで推論時のトークン数を増やすことで、推論時の計算量をスケールさせる
	- Chain-of-Thought
	- Many-Shot ICL
- Decodingを複雑にする
	- Decoding：モデルの出力をどう定義するか、と理解
	- 第3回 Pre-trainingにあったはず

まとめ
- 訓練時だけでなく、推論時にも計算量をスケールさせることで性能を改善することができる
- シンプルな方法
	 - Prompting
	 - Decodingの工夫
- Meta-Generationという枠組みでも説明されるように　**詳細はスライド**
	- Parallel Search
	- Step-Level Search
	- Refinement