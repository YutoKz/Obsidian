#MachineLearning 
## 構成
- 正規分布から拡散モデルへの10ステップ
- ダイジェスト
	- 正規分布
		- $\mu, \sigma$で決定
	- 最尤推定
		- 母集団分布のパラメータを、サンプルから推定
	- 
## ステップ1: 正規分布
- 正規分布は平均μと標準偏差σをパラメータとする確率分布
- 確率密度関数 p(a, b; c, d, e)
	- a, b: 確率変数
	- c, d, e: パラメータ
- 中心極限定理
	- 任意の確率分布(平均$\mu$, 分散$\sigma^2$)からN個サンプリングすることを考える
	- そのサンプル平均 $\bar x$ は、$N$ が十分大きいとき、
	  平均$\mu$, 分散$\frac{\sigma^2}{N}$ の正規分布に従う
	- 
## ステップ2: 最尤推定
- 生成モデル
	- あるデータ$x$の従う確率分布をモデル化
	- そのモデルから新たに疑似的なデータを生成
	  (あたかも、その集団から選んできたかのように。)
- 母集団分布の推定
	1. モデル化
		- 「パラメータで調整可能な確率分布」で母集団分布を近似可能と仮定
		- e.g. 正規分布
	1. パラメータ推定
		- サンプルデータをもとに最適なパラメータを決定
		- e.g. 最尤推定
	- 画像生成モデルのような、複雑な生成モデルであれこの流れは同じ
-  最尤推定
	- 正規分布の場合
		- 母集団の平均と標準偏差 = サンプルのそれ
	- 理論
		- パラメータ$\theta$で決まる確率分布$p(x; \theta)$から得られた$N$個のサンプル列$D$
			- 数式は所与だが$\theta$が未知、これを推定したい
		- $D$の各サンプルは独立して生成。ゆえに$D$の生成確率密度は個別の確率密度の積
		- $p(D; \theta)$: パラメータが$\theta$のとき、サンプル列$D$の得られる確率密度
		- これを、尤度関数$L(\theta)$と定義
		- 最尤推定とは、**尤度関数を最大化する**$\theta$を決定する手法
		- 言い換えれば、サンプルにもっともフィットするモデルを推定
		- あとは尤度関数の微分(偏微分)=0で最大値を決定
## ステップ3: 多次元正規分布
- 多次元正規分布
	- 複数の実数値からなるベクトルに関する正規分布
	- 平均ベクトル$\boldsymbol{\mu}$ と 分散共分散$\boldsymbol{\Sigma}$  で表現
- 共分散
	- 正 => 一方の変数が増加するともう一方の増加
	- 負 => 一方の変数が増加するともう一方は減少
	- 0 => 相関ナシ
- 共分散行列は対称行列
- 多次元正規分布の最尤推定
	- $D$次元のベクトル$x$の正規分布$p(x;\mu, \Sigma)$ から、$N$個のサンプル列$D$ = {$x^{(1)}$, $x^{(2)}$, ... $x^{(N)}$} 
	- $D$が得られる確率密度は$p(D; \mu, \Sigma)$で、個々のサンプルの確率密度の積に等しい。
	- この$p(D; \mu, \Sigma)$を尤度関数と見なし、これを最大化するパラメータ$\mu, \Sigma$を求める。
	- 1次元同様、偏微分=0で求めるが、$\mu, \Sigma$がベクトル/行列であることに注意
## ステップ4: 混合ガウスモデル
- 多峰性分布
	- 複数の山を持つ
- 混合ガウス分布
	- 複数の正規分布が組み合わさった多峰性分布
	- データを生成するには、
		- 生成元の正規分布を選択(ある確率分布に従って)
		- その正規分布からデータ生成
- 混合ガウス分布の生成モデル
	- 混合ガウス分布と仮定
	- パラメータ推定
		- **最尤推定は使用不可** → 解析的に解けない
		- EMアルゴリズム(ステップ5 リンク)
- 混合ガウスモデルの定式化
	- 必要な知識
		- 同時確率
		- 条件付き確率
	- 「$K$個の正規分布からある確率分布に従い1つ選ぶ」
		- 選ぶ確率を表す確率分布、**カテゴリカル分布** $p(z=k; \boldsymbol{\phi})=\phi_k$
	- 「$K$個の正規分布」
		- $\begin{align}\boldsymbol{\mu}=\{\mu_1,\mu_2,...,\mu_K\} \\ \boldsymbol{\Sigma}=\{\Sigma_1,\Sigma_2,...,\Sigma_K\}\end{align}$
		- $p(\boldsymbol{x}|z=k;\mu,\Sigma)=N(\boldsymbol{x};\mu_k,\Sigma_k)$
	- 目標の生成モデル
		- 確率の周辺化により、$p(x)=\sum_{k=1}^Kp(\boldsymbol{x},z=k)$
		- 同時確率 $p(\boldsymbol{x},z=k)=p(z=k)\ p(\boldsymbol{x}|z=k)$
		- よって、$p(x)=\sum_{k=1}^K \phi_kN(\boldsymbol{x};\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})$となり
		  各正規分布にその発生確率$\phi_k$の重みがかかった形
		  すなわち**正規分布の重み付き和**となる
	- 確率変数 $\boldsymbol{x}$ と $z$
		- $\boldsymbol{x}$：直接観測可能
		- $z$：観測不可能 = **潜在変数**
- 最尤推定でなぜ解析的にパラメータ推定できないか
	- 混合ガウスモデル
		- $p(\boldsymbol{x};\boldsymbol{\phi},\boldsymbol{\mu},\boldsymbol{\Sigma})=\sum_{k=1}^K \phi_kN(\boldsymbol{x};\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})$
	- 例によって、尤度$p(D,\boldsymbol{\theta})$はそれぞれのサンプルの確率密度の積で表せる
		- $\boldsymbol{\theta}$：パラメータ$\{\boldsymbol{\phi},\boldsymbol{\mu},\boldsymbol{\Sigma}\}$
	- これも例によって、パラメータ最適化の際はlogをとって対数尤度を最大化するパラメータを求めようとする。しかし、混合ガウスモデルではここでlog-sumの形が出てきてしまい、偏微分=0による解析的なパラメータ推定が困難となる。
	- ここで、EMアルゴリズムが登場
	  今回のような、潜在変数のアルモデルにおいて対数尤度を大きくするパラメータを効率的に探索できる
## ステップ5: EMアルゴリズム
- 準備：$KL$ダイバージェンス
	- 「2つの確率分布がどの程度異なるか」
	- 定義式$$D_{KL}(p||q)=\int p(x)\ log\frac{p(x)}{q(x)}\,dx$$
	- 特徴
		- 0以上、2つの確率分布が一致する場合のみ0
		- 非対称な尺度なので、pとqの順序で値変化
	- 最尤推定との関係
		- 前提：真の確率分布$p_*(x)$をパラメータ付き確率分布$p_\theta(x)$で近似したい
		  $N$個のサンプル列$x^(n)$を使う
		  目的関数である対数尤度$$log\prod_{n=1}^N p_\theta(x^{(n)})=\sum_{n=1}^N log\ p_\theta(x^{(n)})$$
		  これを最大化する$\theta$が目的のパラメータ
		- さて今、目的は「$p_*(x)$に$p_\theta(x)$を近づける」、すなわち
		  「$p_*(x)$と$p_\theta(x)$の$KL$ダイバージェンスを最小化する」こと
		-  $$D_{KL}(p_*\ ||\ p_\theta)=\int p_*(x)\ log\frac{p_*(x)}{p_\theta(x)}dx=E_{p_*(x)}[log\ \frac{p_*(x)}{p_\theta(x)}]$$
		  モンテカルロ法により$$E_{p_*(x)}[log\ \frac{p_*(x)}{p_\theta(x)}]\approx \frac{1}{N}\sum_{n=1}^{N}log\ \frac{p_*(x^{(n)}))}{p_\theta(x^{(n)})}=\frac{1}{N}\sum_{n=1}^{N}(log\ p_*(x^{(n)})-log\  p_\theta(x^{(n)}))$$
		- これを最小化する$\theta$を求めるには、$\sum_{n=1}^{N}log\  p_\theta(x^{(n)})=(対数尤度)$を最大化する$\theta$を考えればよい
		- 従って、
		  「対数尤度の最大化 (**最尤推定**)」と「$KL$ダイバージェンスの最小化」は同義である
- EMアルゴリズムの導出
	- ***詳細は p93 ~*
	- 潜在変数を持つモデル
		- HMM
		- VAE
		- **GMM** (混合ガウスモデル)
	- EMアルゴリズムは、「潜在変数を持つモデル」が対象
	- Expectation - Maximization Algorithm
		- Expectationステップ と Maximizationステップを交互に繰り返してパラメータを更新する
	- 潜在変数が存在する状況で、$N$個のサンプル列$D=\{x^{(n)}\}$ が与えられた場合、その対数尤度は$$log\ p_\theta(D)=\sum_{n=1}^{N} log\ p_\theta(x^{(n)})=\sum_{n=1}^{N} log\ \sum_{z^{(n)}} p_\theta(x^{(n)},z^{(n)})$$
	  となる。
	  潜在変数がなければ2つ目のsum-logで終わるため解析的に解けるが、
	  潜在変数があると周辺化する必要がありlog-sumの形となるため解析的に解けなくなる
	- **ここから理解がまだ甘いので、深まったらまとめよう**
		- キーワード
			- ELBO
				- エビデンス(対数尤度)の下界
				- こいつを大きくすれば対数尤度も大きく
				- 手に負えない対数尤度の代わりにこれを最適化の対象とする
## ステップ6: ニューラルネットワーク
## ステップ7: 変分オートエンコーダ(VAE)
- 7.3 p157 ~ ムズイ。一旦飛ばす。
	- 各工程の目的がいまいちわからん。
	- 全体像に関わる前提知識が足りない...?
	- 他の文献の説明読んでから、出直そう！
## ステップ8: 拡散モデル
- VAEの潜在変数を階層化した「階層型VAE」
- 階層型VAEの発展、「拡散モデル」
- 